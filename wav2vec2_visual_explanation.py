"""
Wav2Vec2 å¯è§†åŒ–åŸç†è§£é‡Š
é€šè¿‡å›¾è¡¨å’Œç¤ºä¾‹æ¥ç†è§£Wav2Vec2çš„å·¥ä½œæœºåˆ¶
"""

import matplotlib.pyplot as plt
import numpy as np
import torch
import matplotlib.patches as patches

def create_wav2vec2_architecture_diagram():
    """åˆ›å»ºWav2Vec2æ¶æ„å›¾"""
    
    fig, ax = plt.subplots(1, 1, figsize=(14, 8))
    
    # å®šä¹‰å„ä¸ªç»„ä»¶çš„ä½ç½®å’Œå¤§å°
    components = [
        {"name": "åŸå§‹éŸ³é¢‘æ³¢å½¢", "pos": (1, 6), "size": (2, 1), "color": "lightblue"},
        {"name": "ç‰¹å¾ç¼–ç å™¨\n(7å±‚1Då·ç§¯)", "pos": (4, 6), "size": (2, 1), "color": "lightgreen"},
        {"name": "ä¸Šä¸‹æ–‡ç½‘ç»œ\n(12å±‚Transformer)", "pos": (7, 6), "size": (2, 1), "color": "lightcoral"},
        {"name": "é‡åŒ–æ¨¡å—\n(Vector Quantizer)", "pos": (10, 6), "size": (2, 1), "color": "lightyellow"},
        {"name": "æŠ•å½±å±‚", "pos": (7, 4), "size": (2, 1), "color": "lightgray"},
        {"name": "å¯¹æ¯”å­¦ä¹ ç›®æ ‡", "pos": (7, 2), "size": (2, 1), "color": "lightpink"}
    ]
    
    # ç»˜åˆ¶ç»„ä»¶
    for comp in components:
        rect = patches.Rectangle(comp["pos"], comp["size"][0], comp["size"][1], 
                               linewidth=2, edgecolor='black', facecolor=comp["color"])
        ax.add_patch(rect)
        ax.text(comp["pos"][0] + comp["size"][0]/2, comp["pos"][1] + comp["size"][1]/2, 
               comp["name"], ha='center', va='center', fontsize=10, weight='bold')
    
    # ç»˜åˆ¶ç®­å¤´è¿æ¥
    arrows = [
        ((3, 6.5), (4, 6.5)),  # åŸå§‹éŸ³é¢‘ -> ç‰¹å¾ç¼–ç å™¨
        ((6, 6.5), (7, 6.5)),  # ç‰¹å¾ç¼–ç å™¨ -> ä¸Šä¸‹æ–‡ç½‘ç»œ
        ((9, 6.5), (10, 6.5)), # ä¸Šä¸‹æ–‡ç½‘ç»œ -> é‡åŒ–æ¨¡å—
        ((8, 6), (8, 5)),      # ä¸Šä¸‹æ–‡ç½‘ç»œ -> æŠ•å½±å±‚
        ((8, 4), (8, 3)),      # æŠ•å½±å±‚ -> å¯¹æ¯”å­¦ä¹ 
    ]
    
    for start, end in arrows:
        ax.annotate('', xy=end, xytext=start,
                   arrowprops=dict(arrowstyle='->', lw=2, color='red'))
    
    # æ·»åŠ ç»´åº¦ä¿¡æ¯
    dimensions = [
        {"pos": (2, 5.5), "text": "[B, 48000]"},
        {"pos": (5, 5.5), "text": "[B, 1199, 768]"},
        {"pos": (8, 5.5), "text": "[B, 1199, 768]"},
        {"pos": (11, 5.5), "text": "[B, 1199, 320]"},
        {"pos": (8, 3.5), "text": "[B, 1199, 256]"},
    ]
    
    for dim in dimensions:
        ax.text(dim["pos"][0], dim["pos"][1], dim["text"], 
               ha='center', va='center', fontsize=9, style='italic', color='blue')
    
    ax.set_xlim(0, 13)
    ax.set_ylim(1, 8)
    ax.set_title('Wav2Vec2 æ¶æ„å›¾', fontsize=16, weight='bold')
    ax.axis('off')
    
    plt.tight_layout()
    plt.savefig('wav2vec2_architecture.png', dpi=300, bbox_inches='tight')
    plt.show()

def visualize_audio_processing_steps():
    """å¯è§†åŒ–éŸ³é¢‘å¤„ç†çš„å„ä¸ªæ­¥éª¤"""
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # 1. åŸå§‹éŸ³é¢‘æ³¢å½¢
    t = np.linspace(0, 1, 1000)
    audio_signal = np.sin(2 * np.pi * 5 * t) + 0.5 * np.sin(2 * np.pi * 10 * t)
    
    axes[0, 0].plot(t, audio_signal)
    axes[0, 0].set_title('1. åŸå§‹éŸ³é¢‘æ³¢å½¢ (16kHzé‡‡æ ·)', fontsize=12, weight='bold')
    axes[0, 0].set_xlabel('æ—¶é—´ (ç§’)')
    axes[0, 0].set_ylabel('å¹…åº¦')
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. ç‰¹å¾ç¼–ç å™¨è¾“å‡ºï¼ˆæ¨¡æ‹Ÿï¼‰
    time_steps = np.arange(0, 50)
    features = np.random.randn(50, 10)  # ç®€åŒ–çš„ç‰¹å¾è¡¨ç¤º
    
    im1 = axes[0, 1].imshow(features.T, aspect='auto', cmap='viridis')
    axes[0, 1].set_title('2. ç‰¹å¾ç¼–ç å™¨è¾“å‡º\n(é™é‡‡æ ·åçš„å±€éƒ¨ç‰¹å¾)', fontsize=12, weight='bold')
    axes[0, 1].set_xlabel('æ—¶é—´æ­¥')
    axes[0, 1].set_ylabel('ç‰¹å¾ç»´åº¦')
    plt.colorbar(im1, ax=axes[0, 1])
    
    # 3. ä¸Šä¸‹æ–‡ç½‘ç»œè¾“å‡ºï¼ˆæ¨¡æ‹Ÿï¼‰
    context_features = np.random.randn(50, 10)
    # æ·»åŠ ä¸€äº›å…¨å±€ç›¸å…³æ€§
    for i in range(1, 50):
        context_features[i] = 0.7 * context_features[i] + 0.3 * context_features[i-1]
    
    im2 = axes[1, 0].imshow(context_features.T, aspect='auto', cmap='plasma')
    axes[1, 0].set_title('3. ä¸Šä¸‹æ–‡ç½‘ç»œè¾“å‡º\n(åŒ…å«å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯)', fontsize=12, weight='bold')
    axes[1, 0].set_xlabel('æ—¶é—´æ­¥')
    axes[1, 0].set_ylabel('ç‰¹å¾ç»´åº¦')
    plt.colorbar(im2, ax=axes[1, 0])
    
    # 4. é‡åŒ–ç»“æœï¼ˆæ¨¡æ‹Ÿï¼‰
    quantized_ids = np.random.randint(0, 20, 50)
    
    axes[1, 1].scatter(range(50), quantized_ids, c=quantized_ids, cmap='tab20', s=50)
    axes[1, 1].set_title('4. é‡åŒ–ç»“æœ\n(ç¦»æ•£çš„éŸ³é¢‘å•å…ƒID)', fontsize=12, weight='bold')
    axes[1, 1].set_xlabel('æ—¶é—´æ­¥')
    axes[1, 1].set_ylabel('é‡åŒ–ID')
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('wav2vec2_processing_steps.png', dpi=300, bbox_inches='tight')
    plt.show()

def explain_contrastive_learning():
    """è§£é‡Šå¯¹æ¯”å­¦ä¹ çš„åŸç†"""
    
    fig, ax = plt.subplots(1, 1, figsize=(12, 8))
    
    # åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„å¯¹æ¯”å­¦ä¹ ç¤ºæ„å›¾
    
    # è¾“å…¥åºåˆ—
    sequence_length = 10
    x_positions = np.arange(sequence_length)
    
    # ç»˜åˆ¶è¾“å…¥åºåˆ—
    for i, x in enumerate(x_positions):
        if i == 5:  # è¢«æ©ç›–çš„ä½ç½®
            color = 'red'
            label = 'MASK' if i == 5 else ''
        else:
            color = 'lightblue'
            label = ''
        
        rect = patches.Rectangle((x, 5), 0.8, 1, facecolor=color, edgecolor='black')
        ax.add_patch(rect)
        
        if label:
            ax.text(x + 0.4, 5.5, label, ha='center', va='center', fontsize=10, weight='bold')
        else:
            ax.text(x + 0.4, 5.5, f't{i}', ha='center', va='center', fontsize=9)
    
    # ç»˜åˆ¶å€™é€‰é¡¹
    candidates = ['æ­£ç¡®', 'é”™è¯¯1', 'é”™è¯¯2', 'é”™è¯¯3']
    colors = ['green', 'gray', 'gray', 'gray']
    
    for i, (cand, color) in enumerate(zip(candidates, colors)):
        rect = patches.Rectangle((2 + i * 2, 2), 1.5, 1, facecolor=color, edgecolor='black', alpha=0.7)
        ax.add_patch(rect)
        ax.text(2.75 + i * 2, 2.5, cand, ha='center', va='center', fontsize=10, weight='bold')
    
    # ç»˜åˆ¶ç®­å¤´
    ax.annotate('', xy=(2.75, 3), xytext=(5.4, 5),
               arrowprops=dict(arrowstyle='->', lw=2, color='green'))
    
    for i in range(1, 4):
        ax.annotate('', xy=(2.75 + i * 2, 3), xytext=(5.4, 5),
                   arrowprops=dict(arrowstyle='->', lw=1, color='red', linestyle='--'))
    
    # æ·»åŠ è¯´æ˜æ–‡å­—
    ax.text(5, 7, 'å¯¹æ¯”å­¦ä¹ ç›®æ ‡ï¼šå­¦ä¹ åŒºåˆ†æ­£ç¡®å’Œé”™è¯¯çš„éŸ³é¢‘è¡¨ç¤º', 
           ha='center', va='center', fontsize=14, weight='bold')
    
    ax.text(1, 0.5, 'æ¨¡å‹éœ€è¦å­¦ä¹ ï¼š\n1. è¢«æ©ç›–ä½ç½®çš„æ­£ç¡®è¡¨ç¤º\n2. åŒºåˆ†çœŸå®å’Œè™šå‡çš„å€™é€‰é¡¹', 
           ha='left', va='center', fontsize=11, 
           bbox=dict(boxstyle="round,pad=0.3", facecolor="lightyellow"))
    
    ax.set_xlim(-1, 11)
    ax.set_ylim(0, 8)
    ax.set_title('Wav2Vec2 å¯¹æ¯”å­¦ä¹ åŸç†', fontsize=16, weight='bold')
    ax.axis('off')
    
    plt.tight_layout()
    plt.savefig('wav2vec2_contrastive_learning.png', dpi=300, bbox_inches='tight')
    plt.show()

def compare_feature_extraction_methods():
    """æ¯”è¾ƒä¸åŒç‰¹å¾æå–æ–¹æ³•"""
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # æ¨¡æ‹ŸéŸ³é¢‘ä¿¡å·
    t = np.linspace(0, 1, 1000)
    signal = np.sin(2 * np.pi * 5 * t) + 0.3 * np.sin(2 * np.pi * 15 * t) + 0.1 * np.random.randn(1000)
    
    # 1. åŸå§‹éŸ³é¢‘
    axes[0, 0].plot(t, signal)
    axes[0, 0].set_title('åŸå§‹éŸ³é¢‘ä¿¡å·', fontsize=12, weight='bold')
    axes[0, 0].set_xlabel('æ—¶é—´ (ç§’)')
    axes[0, 0].set_ylabel('å¹…åº¦')
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. MFCCç‰¹å¾ï¼ˆæ¨¡æ‹Ÿï¼‰
    mfcc_features = np.random.randn(13, 100)  # 13ä¸ªMFCCç³»æ•°ï¼Œ100ä¸ªæ—¶é—´å¸§
    
    im1 = axes[0, 1].imshow(mfcc_features, aspect='auto', cmap='coolwarm')
    axes[0, 1].set_title('MFCCç‰¹å¾\n(æ‰‹å·¥è®¾è®¡çš„é¢‘åŸŸç‰¹å¾)', fontsize=12, weight='bold')
    axes[0, 1].set_xlabel('æ—¶é—´å¸§')
    axes[0, 1].set_ylabel('MFCCç³»æ•°')
    plt.colorbar(im1, ax=axes[0, 1])
    
    # 3. Wav2Vec2ç‰¹å¾ï¼ˆæ¨¡æ‹Ÿï¼‰
    wav2vec_features = np.random.randn(768, 50)  # 768ç»´ç‰¹å¾ï¼Œ50ä¸ªæ—¶é—´æ­¥
    
    im2 = axes[1, 0].imshow(wav2vec_features[:50, :], aspect='auto', cmap='viridis')  # åªæ˜¾ç¤ºå‰50ç»´
    axes[1, 0].set_title('Wav2Vec2ç‰¹å¾\n(å­¦ä¹ å¾—åˆ°çš„è¡¨ç¤º)', fontsize=12, weight='bold')
    axes[1, 0].set_xlabel('æ—¶é—´æ­¥')
    axes[1, 0].set_ylabel('ç‰¹å¾ç»´åº¦ (å‰50ç»´)')
    plt.colorbar(im2, ax=axes[1, 0])
    
    # 4. ç‰¹å¾è´¨é‡å¯¹æ¯”
    methods = ['MFCC', 'Wav2Vec2']
    metrics = ['é²æ£’æ€§', 'ä¸Šä¸‹æ–‡ä¿¡æ¯', 'é€‚åº”æ€§', 'æ€§èƒ½']
    mfcc_scores = [3, 2, 2, 6]
    wav2vec_scores = [8, 9, 9, 9]
    
    x = np.arange(len(metrics))
    width = 0.35
    
    axes[1, 1].bar(x - width/2, mfcc_scores, width, label='MFCC', color='lightcoral')
    axes[1, 1].bar(x + width/2, wav2vec_scores, width, label='Wav2Vec2', color='lightblue')
    
    axes[1, 1].set_title('ç‰¹å¾è´¨é‡å¯¹æ¯”', fontsize=12, weight='bold')
    axes[1, 1].set_xlabel('è¯„ä¼°æŒ‡æ ‡')
    axes[1, 1].set_ylabel('è¯„åˆ† (1-10)')
    axes[1, 1].set_xticks(x)
    axes[1, 1].set_xticklabels(metrics)
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('feature_extraction_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()

def main():
    """è¿è¡Œæ‰€æœ‰å¯è§†åŒ–"""
    
    print("ğŸ¯ ç”ŸæˆWav2Vec2å¯è§†åŒ–å›¾è¡¨...")
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    try:
        print("1. åˆ›å»ºæ¶æ„å›¾...")
        create_wav2vec2_architecture_diagram()
        
        print("2. å¯è§†åŒ–å¤„ç†æ­¥éª¤...")
        visualize_audio_processing_steps()
        
        print("3. è§£é‡Šå¯¹æ¯”å­¦ä¹ ...")
        explain_contrastive_learning()
        
        print("4. æ¯”è¾ƒç‰¹å¾æå–æ–¹æ³•...")
        compare_feature_extraction_methods()
        
        print("âœ… æ‰€æœ‰å›¾è¡¨å·²ç”Ÿæˆå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå›¾è¡¨æ—¶å‡ºé”™: {e}")
        print("ğŸ’¡ æç¤ºï¼šå¦‚æœå­—ä½“é—®é¢˜ï¼Œå›¾è¡¨ä»ä¼šç”Ÿæˆï¼Œåªæ˜¯ä¸­æ–‡å¯èƒ½æ˜¾ç¤ºä¸ºæ–¹å—")

if __name__ == "__main__":
    main() 