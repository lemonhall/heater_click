"""
åŸºäºWav2Vec2çš„çƒ­æ°´å™¨å¼€å…³å£°éŸ³æ£€æµ‹å™¨
ä½¿ç”¨é¢„è®­ç»ƒçš„Wav2Vec2æ¨¡å‹è¿›è¡Œç‰¹å¾æå–ï¼Œç„¶åè®­ç»ƒä¸€ä¸ªç®€å•çš„åˆ†ç±»å™¨
"""

import os
import glob
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchaudio
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import random

# æ£€æŸ¥æ˜¯å¦æœ‰transformersåº“
try:
    from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("âš ï¸  transformersåº“æœªå®‰è£…")
    print("   å®‰è£…å‘½ä»¤: pip install transformers")

class SwitchSoundDataset(Dataset):
    """å¼€å…³å£°éŸ³æ•°æ®é›†"""
    
    def __init__(self, audio_files, labels, feature_extractor, max_length=16000*5):
        self.audio_files = audio_files
        self.labels = labels
        self.feature_extractor = feature_extractor
        self.max_length = max_length
    
    def __len__(self):
        return len(self.audio_files)
    
    def __getitem__(self, idx):
        # åŠ è½½éŸ³é¢‘æ–‡ä»¶
        audio_path = self.audio_files[idx]
        waveform, sample_rate = torchaudio.load(audio_path)
        
        # è½¬æ¢ä¸ºå•å£°é“
        if waveform.shape[0] > 1:
            waveform = waveform.mean(dim=0, keepdim=True)
        
        # é‡é‡‡æ ·åˆ°16kHzï¼ˆå¦‚æœéœ€è¦ï¼‰
        if sample_rate != 16000:
            resampler = torchaudio.transforms.Resample(sample_rate, 16000)
            waveform = resampler(waveform)
        
        # è°ƒæ•´é•¿åº¦
        waveform = waveform.squeeze()
        if len(waveform) > self.max_length:
            # éšæœºè£å‰ª
            start = random.randint(0, len(waveform) - self.max_length)
            waveform = waveform[start:start + self.max_length]
        else:
            # é›¶å¡«å……
            padding = self.max_length - len(waveform)
            waveform = torch.nn.functional.pad(waveform, (0, padding))
        
        # ä½¿ç”¨feature_extractoré¢„å¤„ç†
        inputs = self.feature_extractor(
            waveform.numpy(),
            sampling_rate=16000,
            return_tensors="pt",
            padding=True
        )
        
        return {
            'input_values': inputs.input_values.squeeze(),
            'label': torch.tensor(self.labels[idx], dtype=torch.long)
        }

class Wav2Vec2Classifier(nn.Module):
    """åŸºäºWav2Vec2çš„åˆ†ç±»å™¨"""
    
    def __init__(self, num_classes=2, freeze_wav2vec=True):
        super().__init__()
        
        # åŠ è½½é¢„è®­ç»ƒçš„Wav2Vec2æ¨¡å‹
        self.wav2vec2 = Wav2Vec2Model.from_pretrained("facebook/wav2vec2-base")
        
        # æ˜¯å¦å†»ç»“Wav2Vec2å‚æ•°
        if freeze_wav2vec:
            for param in self.wav2vec2.parameters():
                param.requires_grad = False
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Dropout(0.1),
            nn.Linear(768, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, num_classes)
        )
        
        # å…¨å±€å¹³å‡æ± åŒ–
        self.global_pool = nn.AdaptiveAvgPool1d(1)
    
    def forward(self, input_values):
        # æå–Wav2Vec2ç‰¹å¾
        with torch.no_grad() if hasattr(self, '_freeze_wav2vec') else torch.enable_grad():
            outputs = self.wav2vec2(input_values)
            features = outputs.last_hidden_state  # [batch, time, 768]
        
        # å…¨å±€å¹³å‡æ± åŒ–: [batch, time, 768] -> [batch, 768]
        pooled = features.mean(dim=1)
        
        # åˆ†ç±»
        logits = self.classifier(pooled)
        
        return logits

def generate_negative_samples(positive_files, output_dir="samples_wav", num_negatives=6):
    """
    ç”Ÿæˆè´Ÿæ ·æœ¬ï¼ˆèƒŒæ™¯å™ªéŸ³ï¼‰
    é€šè¿‡å¯¹æ­£æ ·æœ¬è¿›è¡Œå˜æ¢æ¥åˆ›å»ºè´Ÿæ ·æœ¬
    """
    
    print(f"ğŸ”„ ç”Ÿæˆ {num_negatives} ä¸ªè´Ÿæ ·æœ¬...")
    
    negative_files = []
    
    for i in range(num_negatives):
        # éšæœºé€‰æ‹©ä¸€ä¸ªæ­£æ ·æœ¬ä½œä¸ºåŸºç¡€
        base_file = random.choice(positive_files)
        
        try:
            # åŠ è½½éŸ³é¢‘
            waveform, sample_rate = torchaudio.load(base_file)
            waveform = waveform.squeeze()
            
            # åº”ç”¨å˜æ¢ç”Ÿæˆè´Ÿæ ·æœ¬
            if i % 3 == 0:
                # æ–¹æ³•1: æ·»åŠ é«˜æ–¯å™ªéŸ³
                noise = torch.randn_like(waveform) * 0.1
                modified_waveform = noise
            elif i % 3 == 1:
                # æ–¹æ³•2: ä½é€šæ»¤æ³¢ï¼ˆæ¨¡æ‹Ÿè¿œè·ç¦»å£°éŸ³ï¼‰
                modified_waveform = waveform * 0.1 + torch.randn_like(waveform) * 0.05
            else:
                # æ–¹æ³•3: é™éŸ³ + è½»å¾®å™ªéŸ³
                modified_waveform = torch.randn_like(waveform) * 0.02
            
            # ä¿å­˜è´Ÿæ ·æœ¬
            negative_filename = f"background_{i+1:02d}.wav"
            negative_path = os.path.join(output_dir, negative_filename)
            
            torchaudio.save(negative_path, modified_waveform.unsqueeze(0), sample_rate)
            negative_files.append(negative_path)
            
            print(f"âœ… ç”Ÿæˆè´Ÿæ ·æœ¬: {negative_filename}")
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆè´Ÿæ ·æœ¬å¤±è´¥: {e}")
    
    return negative_files

def prepare_dataset():
    """å‡†å¤‡è®­ç»ƒæ•°æ®é›†"""
    
    # æŸ¥æ‰¾æ­£æ ·æœ¬æ–‡ä»¶
    positive_files = glob.glob("samples_wav/switch_on_*.wav")
    
    if not positive_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æ­£æ ·æœ¬æ–‡ä»¶")
        return None, None, None, None
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(positive_files)} ä¸ªæ­£æ ·æœ¬")
    
    # ç”Ÿæˆè´Ÿæ ·æœ¬
    negative_files = generate_negative_samples(positive_files)
    
    # åˆå¹¶æ‰€æœ‰æ–‡ä»¶å’Œæ ‡ç­¾
    all_files = positive_files + negative_files
    all_labels = [1] * len(positive_files) + [0] * len(negative_files)
    
    print(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    print(f"   æ­£æ ·æœ¬: {len(positive_files)} ä¸ª")
    print(f"   è´Ÿæ ·æœ¬: {len(negative_files)} ä¸ª")
    print(f"   æ€»è®¡: {len(all_files)} ä¸ª")
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    train_files, test_files, train_labels, test_labels = train_test_split(
        all_files, all_labels, test_size=0.3, random_state=42, stratify=all_labels
    )
    
    return train_files, test_files, train_labels, test_labels

def train_model(train_dataset, test_dataset, num_epochs=20):
    """è®­ç»ƒæ¨¡å‹"""
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)
    
    # åˆ›å»ºæ¨¡å‹
    model = Wav2Vec2Classifier(num_classes=2)
    model.to(device)
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)
    
    # è®­ç»ƒå†å²
    train_losses = []
    train_accuracies = []
    test_accuracies = []
    
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        for batch in train_loader:
            input_values = batch['input_values'].to(device)
            labels = batch['label'].to(device)
            
            optimizer.zero_grad()
            outputs = model(input_values)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            train_total += labels.size(0)
            train_correct += (predicted == labels).sum().item()
        
        train_accuracy = 100 * train_correct / train_total
        avg_train_loss = train_loss / len(train_loader)
        
        # æµ‹è¯•é˜¶æ®µ
        model.eval()
        test_correct = 0
        test_total = 0
        
        with torch.no_grad():
            for batch in test_loader:
                input_values = batch['input_values'].to(device)
                labels = batch['label'].to(device)
                
                outputs = model(input_values)
                _, predicted = torch.max(outputs.data, 1)
                test_total += labels.size(0)
                test_correct += (predicted == labels).sum().item()
        
        test_accuracy = 100 * test_correct / test_total
        
        # è®°å½•å†å²
        train_losses.append(avg_train_loss)
        train_accuracies.append(train_accuracy)
        test_accuracies.append(test_accuracy)
        
        print(f"Epoch [{epoch+1}/{num_epochs}] - "
              f"Loss: {avg_train_loss:.4f}, "
              f"Train Acc: {train_accuracy:.2f}%, "
              f"Test Acc: {test_accuracy:.2f}%")
    
    return model, train_losses, train_accuracies, test_accuracies

def evaluate_model(model, test_dataset):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)
    
    model.eval()
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        for batch in test_loader:
            input_values = batch['input_values'].to(device)
            labels = batch['label'].to(device)
            
            outputs = model(input_values)
            _, predicted = torch.max(outputs.data, 1)
            
            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(all_labels, all_predictions)
    
    print(f"\nğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ:")
    print(f"å‡†ç¡®ç‡: {accuracy:.4f}")
    print(f"\nåˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(all_labels, all_predictions, 
                              target_names=['æ— å¼€å…³', 'æœ‰å¼€å…³']))
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(all_labels, all_predictions)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['æ— å¼€å…³', 'æœ‰å¼€å…³'],
                yticklabels=['æ— å¼€å…³', 'æœ‰å¼€å…³'])
    plt.title('æ··æ·†çŸ©é˜µ')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.tight_layout()
    plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return accuracy

def save_model(model, filepath="switch_detector_model.pth"):
    """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
    torch.save({
        'model_state_dict': model.state_dict(),
        'model_class': 'Wav2Vec2Classifier'
    }, filepath)
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")

def main():
    """ä¸»å‡½æ•°"""
    
    if not TRANSFORMERS_AVAILABLE:
        print("âŒ éœ€è¦å®‰è£…transformersåº“æ‰èƒ½ä½¿ç”¨Wav2Vec2")
        return
    
    print("ğŸ¯ çƒ­æ°´å™¨å¼€å…³å£°éŸ³æ£€æµ‹å™¨")
    print("=" * 50)
    
    # å‡†å¤‡æ•°æ®é›†
    train_files, test_files, train_labels, test_labels = prepare_dataset()
    
    if train_files is None:
        return
    
    # åˆ›å»ºfeature extractor
    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained("facebook/wav2vec2-base")
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = SwitchSoundDataset(train_files, train_labels, feature_extractor)
    test_dataset = SwitchSoundDataset(test_files, test_labels, feature_extractor)
    
    print(f"\nğŸ“Š æ•°æ®é›†åˆ’åˆ†:")
    print(f"   è®­ç»ƒé›†: {len(train_dataset)} ä¸ªæ ·æœ¬")
    print(f"   æµ‹è¯•é›†: {len(test_dataset)} ä¸ªæ ·æœ¬")
    
    # è®­ç»ƒæ¨¡å‹
    model, train_losses, train_accuracies, test_accuracies = train_model(
        train_dataset, test_dataset, num_epochs=15
    )
    
    # è¯„ä¼°æ¨¡å‹
    accuracy = evaluate_model(model, test_dataset)
    
    # ä¿å­˜æ¨¡å‹
    save_model(model)
    
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print(f"   æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {accuracy:.4f}")
    print(f"   æ¨¡å‹å·²ä¿å­˜ï¼Œå¯ç”¨äºå®æ—¶æ£€æµ‹")

if __name__ == "__main__":
    main() 