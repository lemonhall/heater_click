"""
å®æ—¶éº¦å…‹é£çƒ­æ°´å™¨å¼€å…³å£°éŸ³æ£€æµ‹å™¨
ä½¿ç”¨è®­ç»ƒå¥½çš„Wav2Vec2æ¨¡å‹è¿›è¡Œå®æ—¶æ£€æµ‹
"""

import torch
import torch.nn as nn
import numpy as np
import pyaudio
import threading
import time
from collections import deque
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from datetime import datetime
import queue
import warnings
warnings.filterwarnings("ignore")

# æ£€æŸ¥ä¾èµ–
try:
    from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("âŒ éœ€è¦å®‰è£…transformersåº“")

try:
    import pyaudio
    PYAUDIO_AVAILABLE = True
except ImportError:
    PYAUDIO_AVAILABLE = False
    print("âŒ éœ€è¦å®‰è£…pyaudioåº“")
    print("   å®‰è£…å‘½ä»¤: pip install pyaudio")

class Wav2Vec2Classifier(nn.Module):
    """åŸºäºWav2Vec2çš„åˆ†ç±»å™¨ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒçš„æ¶æ„ï¼‰"""
    
    def __init__(self, num_classes=2, freeze_wav2vec=True):
        super().__init__()
        
        # åŠ è½½é¢„è®­ç»ƒçš„Wav2Vec2æ¨¡å‹
        self.wav2vec2 = Wav2Vec2Model.from_pretrained("facebook/wav2vec2-base")
        
        # æ˜¯å¦å†»ç»“Wav2Vec2å‚æ•°
        if freeze_wav2vec:
            for param in self.wav2vec2.parameters():
                param.requires_grad = False
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Dropout(0.1),
            nn.Linear(768, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, num_classes)
        )
        
        # å…¨å±€å¹³å‡æ± åŒ–
        self.global_pool = nn.AdaptiveAvgPool1d(1)
    
    def forward(self, input_values):
        # æå–Wav2Vec2ç‰¹å¾
        with torch.no_grad():
            outputs = self.wav2vec2(input_values)
            features = outputs.last_hidden_state  # [batch, time, 768]
        
        # å…¨å±€å¹³å‡æ± åŒ–: [batch, time, 768] -> [batch, 768]
        pooled = features.mean(dim=1)
        
        # åˆ†ç±»
        logits = self.classifier(pooled)
        
        return logits

class RealtimeSwitchDetector:
    """å®æ—¶å¼€å…³å£°éŸ³æ£€æµ‹å™¨"""
    
    def __init__(self, model_path="switch_detector_model.pth", 
                 sample_rate=16000, chunk_size=1024, 
                 detection_window=3.0, confidence_threshold=0.93):
        
        self.sample_rate = sample_rate
        self.chunk_size = chunk_size
        self.detection_window = detection_window
        self.confidence_threshold = confidence_threshold
        
        # éŸ³é¢‘ç¼“å†²åŒº
        self.audio_buffer = deque(maxlen=int(sample_rate * detection_window))
        
        # æ£€æµ‹ç»“æœé˜Ÿåˆ—
        self.detection_queue = queue.Queue()
        
        # æ§åˆ¶æ ‡å¿—
        self.is_running = False
        self.is_detecting = False
        
        # åŠ è½½æ¨¡å‹
        self.load_model(model_path)
        
        # åˆå§‹åŒ–éŸ³é¢‘
        self.init_audio()
        
        # æ£€æµ‹å†å²
        self.detection_history = deque(maxlen=100)
        self.last_detection_time = 0
        
        print("ğŸ¯ å®æ—¶å¼€å…³å£°éŸ³æ£€æµ‹å™¨å·²åˆå§‹åŒ–")
        print(f"   é‡‡æ ·ç‡: {sample_rate} Hz")
        print(f"   æ£€æµ‹çª—å£: {detection_window} ç§’")
        print(f"   ç½®ä¿¡åº¦é˜ˆå€¼: {confidence_threshold}")
    
    def load_model(self, model_path):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        try:
            # åŠ è½½æ¨¡å‹
            checkpoint = torch.load(model_path, map_location='cpu')
            self.model = Wav2Vec2Classifier(num_classes=2)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model.eval()
            
            # åŠ è½½ç‰¹å¾æå–å™¨
            self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained("facebook/wav2vec2-base")
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def init_audio(self):
        """åˆå§‹åŒ–éŸ³é¢‘è¾“å…¥"""
        if not PYAUDIO_AVAILABLE:
            raise ImportError("éœ€è¦å®‰è£…pyaudioåº“")
        
        self.audio = pyaudio.PyAudio()
        
        # æŸ¥æ‰¾å¯ç”¨çš„éŸ³é¢‘è®¾å¤‡
        self.list_audio_devices()
        
        # æ‰“å¼€éŸ³é¢‘æµ
        self.stream = self.audio.open(
            format=pyaudio.paFloat32,
            channels=1,
            rate=self.sample_rate,
            input=True,
            frames_per_buffer=self.chunk_size,
            stream_callback=self.audio_callback
        )
        
        print("ğŸ¤ éŸ³é¢‘è¾“å…¥å·²åˆå§‹åŒ–")
    
    def list_audio_devices(self):
        """åˆ—å‡ºå¯ç”¨çš„éŸ³é¢‘è®¾å¤‡"""
        print("\nğŸ¤ å¯ç”¨éŸ³é¢‘è®¾å¤‡:")
        for i in range(self.audio.get_device_count()):
            info = self.audio.get_device_info_by_index(i)
            if info['maxInputChannels'] > 0:
                print(f"   è®¾å¤‡ {i}: {info['name']} (è¾“å…¥é€šé“: {info['maxInputChannels']})")
    
    def audio_callback(self, in_data, frame_count, time_info, status):
        """éŸ³é¢‘å›è°ƒå‡½æ•°"""
        if self.is_running:
            # è½¬æ¢éŸ³é¢‘æ•°æ®
            audio_data = np.frombuffer(in_data, dtype=np.float32)
            
            # æ·»åŠ åˆ°ç¼“å†²åŒº
            self.audio_buffer.extend(audio_data)
            
            # å¦‚æœç¼“å†²åŒºæ»¡äº†ï¼Œè¿›è¡Œæ£€æµ‹
            if len(self.audio_buffer) >= int(self.sample_rate * self.detection_window):
                if not self.is_detecting:
                    # å¯åŠ¨æ£€æµ‹çº¿ç¨‹
                    detection_thread = threading.Thread(target=self.detect_switch)
                    detection_thread.daemon = True
                    detection_thread.start()
        
        return (in_data, pyaudio.paContinue)
    
    def detect_switch(self):
        """æ£€æµ‹å¼€å…³å£°éŸ³"""
        if self.is_detecting:
            return
        
        self.is_detecting = True
        
        try:
            # è·å–éŸ³é¢‘æ•°æ®
            audio_data = np.array(list(self.audio_buffer))
            
            # ç¡®ä¿é•¿åº¦æ­£ç¡®
            target_length = int(self.sample_rate * self.detection_window)
            if len(audio_data) > target_length:
                audio_data = audio_data[-target_length:]
            elif len(audio_data) < target_length:
                # é›¶å¡«å……
                padding = target_length - len(audio_data)
                audio_data = np.pad(audio_data, (0, padding), 'constant')
            
            # é¢„å¤„ç†éŸ³é¢‘
            inputs = self.feature_extractor(
                audio_data,
                sampling_rate=self.sample_rate,
                return_tensors="pt",
                padding=True
            )
            
            # æ¨¡å‹æ¨ç†
            with torch.no_grad():
                outputs = self.model(inputs.input_values)
                probabilities = torch.softmax(outputs, dim=1)
                confidence = probabilities[0][1].item()  # å¼€å…³å£°éŸ³çš„æ¦‚ç‡
                prediction = 1 if confidence > self.confidence_threshold else 0
            
            # è®°å½•æ£€æµ‹ç»“æœ
            current_time = time.time()
            result = {
                'timestamp': current_time,
                'prediction': prediction,
                'confidence': confidence,
                'datetime': datetime.now().strftime("%H:%M:%S")
            }
            
            # æ·»åŠ åˆ°å†å²è®°å½•
            self.detection_history.append(result)
            
            # å¦‚æœæ£€æµ‹åˆ°å¼€å…³å£°éŸ³
            if prediction == 1:
                # é¿å…é‡å¤æ£€æµ‹ï¼ˆ2ç§’å†…åªæŠ¥å‘Šä¸€æ¬¡ï¼‰
                if current_time - self.last_detection_time > 2.0:
                    self.last_detection_time = current_time
                    print(f"ğŸ”” [{result['datetime']}] æ£€æµ‹åˆ°å¼€å…³å£°éŸ³! ç½®ä¿¡åº¦: {confidence:.3f}")
                    
                    # æ·»åŠ åˆ°æ£€æµ‹é˜Ÿåˆ—
                    self.detection_queue.put(result)
            
        except Exception as e:
            print(f"âŒ æ£€æµ‹è¿‡ç¨‹å‡ºé”™: {e}")
        
        finally:
            self.is_detecting = False
    
    def start_detection(self):
        """å¼€å§‹æ£€æµ‹"""
        print("\nğŸš€ å¼€å§‹å®æ—¶æ£€æµ‹...")
        print("æŒ‰ Ctrl+C åœæ­¢æ£€æµ‹")
        
        self.is_running = True
        self.stream.start_stream()
        
        try:
            while self.is_running:
                time.sleep(0.1)
                
                # å¤„ç†æ£€æµ‹ç»“æœé˜Ÿåˆ—
                try:
                    while not self.detection_queue.empty():
                        result = self.detection_queue.get_nowait()
                        # è¿™é‡Œå¯ä»¥æ·»åŠ å…¶ä»–å¤„ç†é€»è¾‘ï¼Œæ¯”å¦‚å‘é€é€šçŸ¥ç­‰
                        pass
                except queue.Empty:
                    pass
                    
        except KeyboardInterrupt:
            print("\nâ¹ï¸  æ£€æµ‹å·²åœæ­¢")
        finally:
            self.stop_detection()
    
    def stop_detection(self):
        """åœæ­¢æ£€æµ‹"""
        self.is_running = False
        
        if hasattr(self, 'stream'):
            self.stream.stop_stream()
            self.stream.close()
        
        if hasattr(self, 'audio'):
            self.audio.terminate()
        
        print("ğŸ”š æ£€æµ‹å™¨å·²å…³é—­")
    
    def get_detection_stats(self):
        """è·å–æ£€æµ‹ç»Ÿè®¡ä¿¡æ¯"""
        if not self.detection_history:
            return "æš‚æ— æ£€æµ‹æ•°æ®"
        
        total_detections = len(self.detection_history)
        switch_detections = sum(1 for d in self.detection_history if d['prediction'] == 1)
        
        recent_detections = [d for d in self.detection_history 
                           if time.time() - d['timestamp'] < 60]  # æœ€è¿‘1åˆ†é’Ÿ
        recent_switch_count = sum(1 for d in recent_detections if d['prediction'] == 1)
        
        stats = f"""
ğŸ“Š æ£€æµ‹ç»Ÿè®¡ (æœ€è¿‘1åˆ†é’Ÿ):
   æ€»æ£€æµ‹æ¬¡æ•°: {len(recent_detections)}
   å¼€å…³æ£€æµ‹æ¬¡æ•°: {recent_switch_count}
   å¹³å‡ç½®ä¿¡åº¦: {np.mean([d['confidence'] for d in recent_detections]):.3f}
        """
        
        return stats

def main():
    """ä¸»å‡½æ•°"""
    
    if not TRANSFORMERS_AVAILABLE or not PYAUDIO_AVAILABLE:
        print("âŒ ç¼ºå°‘å¿…è¦çš„ä¾èµ–åº“")
        return
    
    print("ğŸ¯ çƒ­æ°´å™¨å¼€å…³å£°éŸ³å®æ—¶æ£€æµ‹å™¨")
    print("=" * 50)
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    model_path = "switch_detector_model.pth"
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        print("   è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬ç”Ÿæˆæ¨¡å‹")
        return
    
    try:
        # åˆ›å»ºæ£€æµ‹å™¨
        detector = RealtimeSwitchDetector(
            model_path=model_path,
            confidence_threshold=0.93  # è°ƒæ•´ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œåªæœ‰é«˜ç½®ä¿¡åº¦æ‰è®¤ä¸ºæ˜¯å¼€å…³å£°éŸ³
        )
        
        # å¼€å§‹æ£€æµ‹
        detector.start_detection()
        
    except Exception as e:
        print(f"âŒ æ£€æµ‹å™¨å¯åŠ¨å¤±è´¥: {e}")
    
    print("\nğŸ‘‹ å†è§!")

if __name__ == "__main__":
    import os
    main() 